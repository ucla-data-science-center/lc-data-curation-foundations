<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Introduction to Data Curation for Reproducibility: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../favicons/lc/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicons/lc/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicons/lc/favicon-16x16.png">
<link rel="manifest" href="../favicons/lc/site.webmanifest">
<link rel="mask-icon" href="../favicons/lc/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav library"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="lc-logo" alt="Lesson Description" src="../assets/images/library-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav library" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Lesson Description" src="../assets/images/library-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Introduction to Data Curation for Reproducibility
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Introduction to Data Curation for Reproducibility
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Introduction to Data Curation for Reproducibility
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress library">
    <div class="progress-bar library" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="01-overview.html">1. Overview of Scientific Reproducibility</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="02-standards.html">2. Reproducibility Standards</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="03-LIS.html">3. Scientific Reproducibility and the LIS Professional</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="discuss.html">Discussion</a></li>
<li><a class="dropdown-item" href="reference.html">Glossary</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-01-overview"><p>Content from <a href="01-overview.html">Overview of Scientific Reproducibility</a></p>
<hr>
<p>Last updated on 2024-11-25 |

        <a href="https://github.com/ucla-data-science-center/lc-cure-01/edit/main/episodes/01-overview.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 45 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How is ‘reproducibility’ defined?</li>
<li>Why is scientific reproducibility important?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Differentiate among various definitions of ‘reproducibility’ and
other related concepts.</li>
<li>Recall historical and contemporary imperatives for scientific
reproducibility.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>Anyone working in an organization that supports or engages in
scientific research has likely encountered discussions about
reproducibility. In the past decade, various research stakeholders have
become more and more insistent in their calls for data sharing and
research transparency as a means of upholding the credibility of the
scientific outputs. This episode engages in these discussions and
illustrates the various reasons behind demands for reproducibility,
beginning with an exploration of the term, “reproducibility.”</p>
<section><h2 class="section-heading" id="defining-reproducibility">Defining “Reproducibility”<a class="anchor" aria-label="anchor" href="#defining-reproducibility"></a>
</h2>
<hr class="half-width">
<p>Reproducibility, which has emerged as a standard-bearer for
scientific integrity, considers what happens when a research study is
repeated. For example, a researcher discovers an article that reports
study findings, and then decides to conduct the same study to test
whether or not the results will be consistent in a subsequent iteration
of the study. If the results from the repeated study corroborate those
of the original study, then the results in the published article can be
considered reproducible.</p>
<p>However, the term “reproducibility” carries important assumptions
about the nature of the repeated study. In some contexts, the assumption
is that the repeated study applied the same methods and analyzed the
same data used by the original study investigator. In other contexts,
reproducibility implies that either the original methods or the original
data, but not both, were used to assess the integrity of results.</p>
<p>To disambiguate these various meanings, we specify three types of
reproducibility that articulate the primary distinctions among the
various conceptions of reproducibility defined by <a href="https://doi.org/10.1146/annurev-statistics-010814-020127" class="external-link">Victoria
Stodden (2015)</a>: <em>empirical reproducibility</em>, <em>statistical
reproducibility</em>, and <em>computational reproducibility</em>.</p>
<div class="section level3">
<h3 id="empirical-reproducibility">Empirical Reproducibility<a class="anchor" aria-label="anchor" href="#empirical-reproducibility"></a>
</h3>
<p><strong>original methods + new data</strong></p>
<p>Empirical reproducibility assumes that there is comprehensive
documentation of research methods and protocols to follow, and this
allows for subsequent studies to follow those same research methods and
protocols to collect and analyze data to yield results that corroborate
those reported by the original investigators.</p>
<p>In animal studies, repeating experiments to demonstrate empirical
reproducibility is expected. This requires standardization of
experimental conditions to ensure that inconsistencies in study
protocols are not the cause of inconsistencies in results. For mice
experiments, this includes the use of lab-specified cage types and
temperature, cage cleaning schedules, feeding protocols, etc. Despite
careful adherence to standards, it is known that one often overlooked
detail can affect the empirical reproducibility of experimental
results–the experimenter. For reasons yet to be fully understood,
differences among experimenters such as gender and attitude may
introduce enough variance in experimental conditions that are the cause
of empirical irreproducibility.</p>
</div>
<div class="section level3">
<h3 id="statistical-reproducibility">Statistical Reproducibility<a class="anchor" aria-label="anchor" href="#statistical-reproducibility"></a>
</h3>
<p><strong>new methods + original data</strong></p>
<p>Statistical reproducibility relies on the application of appropriate
statistical methods to yield defensible results. Studies that suffer
from statistical irreproducibility have weaknesses in the statistical
methods and analysis, which precludes the possibility of repeating the
study and yielding the same results.</p>
<p>Because of the impossibility of studying perpetually-changing
conditions of the Earth’s climate system, climate scientists test the
reliability of their global temperature projections using a coordinated
model intercomparison strategy. With this strategy, researchers run
different computational models using the same data inputs and then
compare the results produced by each model. Agreement among models
demonstrates statistical reproducibility, which suggests the reliability
of results. Disagreement, on the other hand, may indicate misapplication
of model algorithms or problematic model design indicative of
statistical irreproducibility.</p>
</div>
<div class="section level3">
<h3 id="computational-reproducibility">Computational Reproducibility<a class="anchor" aria-label="anchor" href="#computational-reproducibility"></a>
</h3>
<p><strong>original methods + original data</strong></p>
<p>Computational reproducibility considers scientific practice that
relies on computational methods to produce results. Simply put, it is
the ability to use the original data and code to produce identical
results as those of the original study.</p>
<p>The discovery of a single flaw in programming scripts used to analyze
chemistry data called into question over 100 published studies. The
issue was a module included in the script that required use of a
specific operating system to execute calculations to produce the correct
results. Researchers realized this only when they noticed inconsistent
results when running the scripts in different computing environments.
The inconsistencies, which demonstrate computational irreproducibility,
were significant enough to prompt researchers to reconsider the results
of their published studies. <br><br><strong>Suggested
reading:</strong><br>
Stodden, V. (2015). Reproducing statistical results. <em>Annual Review
of Statistics and Its Application, 2</em>(1), 1–19. <a href="https://doi.org/10.1146/annurev-statistics-010814-020127" class="external-link uri">https://doi.org/10.1146/annurev-statistics-010814-020127</a></p>
<div id="spotlight-reproducibility-vs.-replicability" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-reproducibility-vs.-replicability" class="callout-inner">
<h3 class="callout-title">Spotlight: Reproducibility vs. Replicability</h3>
<div class="callout-content">
<p>In discussions of reproducibility, you also may have seen the term
replicability used to describe any one of the three definitions provided
above. The terms reproducibility and replicability have been used
interchangeably and with conflating definitions, ultimately obfuscating
their intended meanings. However, it does seem that the scientific
community is converging on clearer definitions that capture the nuances
of these related terms.</p>
<p>A recent report published by the National Academies of Sciences,
Engineering and Medicine (2019) makes a clear distinction between
reproducibility and replicability as a means to resolve ambiguities in
the use of these and related terms. Their definitions are below as
presented in the report:<br><br></p>
<blockquote>
<p><em>Reproducibility</em> is obtaining consistent results using the
same input data; computational steps, methods, and code; and conditions
of analysis. This definition is synonymous with “computational
reproducibility[.]”</p>
<p><em>Replicability</em> is obtaining consistent results across studies
aimed at answering the same scientific question, each of which has
obtained its own data (p. 46).</p>
</blockquote>
<p><strong>Suggested reading:</strong><br>
National Academies of Sciences, Engineering, and Medicine. (2019).
<em>Reproducibility and replicability in science</em>. National
Academies Press. <a href="https://doi.org/10.17226/25303" class="external-link uri">https://doi.org/10.17226/25303</a></p>
</div>
</div>
</div>
<p>The following challenge offers an opportunity to apply the
definitions of the three types of reproducibility to real-world
scenarios. Exploring these scenarios will help to solidify our
understanding of the nuances among the reproducibility types and their
definitions, and to see how irreproducibility can play out.</p>
<div id="exercise-reproducibility-reproducibility-reproducibility" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div id="exercise-reproducibility-reproducibility-reproducibility" class="callout-inner">
<h3 class="callout-title">Exercise: Reproducibility, Reproducibility, Reproducibility</h3>
<div class="callout-content">
<p>Read the descriptions of real-life instances in which published
research findings were discovered to be non-reproducible. Based on the
definitions of reproducibility types, determine
which—<strong>empirical</strong>, <strong>statistical</strong>, or
<strong>computational</strong>—applies to the scenario in each
description.</p>
<p><strong>Scenario 1: Slicing and Dicing Food Data</strong><br>
If you believe that shopping hungry compels you to make poor food
purchasing decisions or that the size of your plate can influence how
much you eat, you have a single university-based food lab to thank. Over
the past three decades, the principal investigator (PI) of this renowned
lab had over 250 articles published describing results of studies of
food-related behavior, many of which have entered the popular
imagination through mainstream media outlets. Some of these studies
informed the USDA’s Center for Nutrition Policy and Promotion, which
issues dietary guidance for the National School Lunch Program,
Supplemental Nutrition Assistance Program (SNAP), and other federally
sponsored nutrition programs.</p>
<p>A closer look at the lab’s research by other scientists and
statisticians, however, revealed considerable problems that would
prevent them from repeating the analyses of published works. What they
discovered was rampant p-hacking, a dubious practice in which
statistical tests are run repeatedly on a dataset in an attempt to yield
statistically significant results, irrespective of the research question
or hypothesis—if any—at hand. Beyond p-hacking, scientists also found
calculation errors including unfeasible means and standard deviations.
This was, perhaps, no accident, as evidenced by an email from the PI to
a lab member that instructed her to dig through a dataset to find
significant relationships among variables, writing, “I don’t think I’ve
ever done an interesting study where the data ‘came out’ the first time
I looked at it.” A few years later after these discoveries were made,
several of the PI’s publications were retracted and an academic
misconduct investigation prompted his removal from teaching and research
duties at the university.</p>
<p><strong>Scenario 2: Excel Fail</strong><br>
In 2013, a graduate student at the University of Massachusetts Amherst
was assigned a class project to select an article from an economics
journal and attempt to reproduce its findings. After several attempts,
the student was unsuccessful, assuming that he had made some mistake in
his own analysis. Even after obtaining the underlying analysis data from
the well-respected authors themselves, he was still unable to reproduce
the results. Upon closer inspection of the spreadsheet of data he was
given, however, he found a critical discovery that would shock the
entire field of economics.</p>
<p>That discovery was a basic Excel miscalculation, one that tested the
veracity of the findings from the influential article that was cited
over 4,000 times and used by governments to justify specific austerity
measures to address economic crises. While corrections to the analysis
yielded different results (which the student later published) from those
in the original article, the authors insisted that these differences did
not change their central findings. They did, however, acknowledge their
error in a public statement: “It is sobering that such an error slipped
into one of our papers despite our best efforts to be consistently
careful. We will redouble our efforts to avoid such errors in the
future.”</p>
<p><strong>Scenario 3: Power(less) Pose</strong><br>
With over 60,000 online views, one of the most popular TED Talks
convinced many people that “power posing” induces an actual sense of
increased power. The presenter, a social psychologist, shared the
results of a study she published along with two of her colleagues that
investigated the physiological and behavioral effects of posing in open,
expansive postures: wide stance, hands on hips. The authors found that
embodiments of power (i.e., power poses) cause a decrease of cortisol
(the stress hormone) while increasing testosterone and risk tolerance.
Based on these observations, they concluded that power posing manifests
actual feelings of power.</p>
<p>After other scientists repeated the study and failed to yield
comparable results that could demonstrate the robustness of the original
power pose findings, some in the scientific community cast doubt on the
soundness of the original study design and protocols. One of the most
vocal skeptics was one of the co-authors of the original study, who
later announced that “I do not believe that ‘power pose’ effects are
real.” Aside from the small sample size and a questionable participant
selection process, he explained that the experiment involved male and
female participants (not distinguished by gender) who performed
risk-taking tasks and subsequently were informed that they had won small
cash prizes. Because the act of winning increases testosterone levels,
there was no way to determine whether the increase in testosterone was
an effect of the power pose as the paper concluded, or if it was merely
an effect of winning. Despite this controversy, studies on power posing
continue–with varying results.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p><strong>Scenario 1 (Slicing and Dicing Food Data): Statistical
Reproducibility</strong><br>
In this scenario, research results were generated using an
inappropriate, dubious statistical technique. Statistical analyses were
also rife with errors and miscalculations, which would make it
impossible for anyone to arrive at the same results. This example
demonstrates lack of <em>statistical reproducibility</em>.</p>
<p><strong>Scenario 2 (Excel Fail): Computational
Reproducibility</strong><br>
In this scenario, the student attempted to reproduce the findings in a
journal article by using the authors’ own data used to generate the
results reported in the article. Unfortunately, Herndon was unable to do
so, which demonstrates a failure to <em>computationally reproduce</em>
the published results.</p>
<p><strong>Scenario 3 (Power(less) Pose): Empirical
Reproducibility</strong><br>
In this scenario, researchers repeated the steps of the original power
pose experiment but arrived at different results. Problems with the
study design and protocols rendered the original power pose study
<em>empirically irreproducible</em>.</p>
</div>
</div>
</div>
</div>
<div id="spotlight-what-about-qualitative-research" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-what-about-qualitative-research" class="callout-inner">
<h3 class="callout-title">Spotlight: What About Qualitative Research?</h3>
<div class="callout-content">
<p>Qualitative research methodologies do not rely on analyses of
quantitatively measurable variables (other than research that transforms
qualitatively derived data into quantitatively analyzable data, e.g.,
text mining, natural language processing), which is why some scholars
have argued that reproducibility standards do not or cannot apply to
qualitative research. There is no doubt, however, that findings from
qualitative studies must be subject to the same level of scrutiny as
those produced by their quantitative counterparts. Certainly, the
credibility of qualitative research is just as important as the
credibility of quantitative research. Assessing the reproducibility of
qualitative research is conceivable, albeit with a focus on
<strong>transparency</strong>.</p>
<p>Considering the more constructivist nature of most qualitative
research (in contrast to positivist quantitative research), assessments
of research integrity emphasize <em>production transparency</em> and
<em>analytic transparency</em>. Production transparency requires that
the processes for research participant selection, data collection, and
analytic interpretation—and the decisions that informed these
processes—be documented in explicit detail. Analytic transparency
demands clear descriptions and explanations of the methods and logic
used to draw conclusions from the data. For qualitative research, a high
degree of production and analytic transparency is prerequisite for
demonstrating research rigor and credibility.</p>
<p><br><strong>Suggested readings:</strong><br> Elman, C., Kapiszewski,
D., &amp; Lupia, A. (2018). Transparent social inquiry: Implications for
social science. <em>Annual Review of Political Science, 21</em>, 29–47.
<a href="https://doi.org/10.1146/annurev-polisci-091515-025429" class="external-link">https://doi.org/10.1146/annurev-polisci-091515-025429</a></p>
<p>TalkadSukumar, P., &amp; Metoyer, R. (2019). Replication and
transparency of qualitative research from a constructivist perspective
[Preprint]. Open Science Framework. <a href="https://doi.org/10.31219/osf.io/6efvp" class="external-link uri">https://doi.org/10.31219/osf.io/6efvp</a></p>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-impetus-for-scientific-reproducibility">The Impetus for Scientific Reproducibility<a class="anchor" aria-label="anchor" href="#the-impetus-for-scientific-reproducibility"></a>
</h2>
<hr class="half-width">
<p>To appreciate the importance of scientific reproducibility, consider
the belief that autism is linked to vaccines. This notion first appeared
in a 1998 article by Andrew Wakefield and his colleagues, who reported
results of a study of a small group of children who had received the MMR
vaccine and were later diagnosed with autism. A subsequent examination
of the study protocols and data from health records, however, revealed
that no interpretation of the data could have reasonably concluded that
instances of autism diagnoses were linked to the vaccine. The Wakefield
articles eventually were retracted by the <em>Lancet</em> journal that
published them, and Wakefield was found guilty of scientific misconduct
and fraud. Our understanding of the relationship between autism and
vaccines is more reliably supported by the many studies and
meta-analyses of studies on the subject that have consistently shown
that vaccines do not cause autism.</p>
<p>For scientific claims to be credible, they must be able to stand up
to scrutiny, which is a hallmark of science. The scientific community
promotes the credibility of its claims through systems of peer review
and research replication. Confirmation of the reproducibility of
research results adds another necessary element of research checks and
balances, particularly with scientific research increasingly becoming
computationally intensive. <strong>The ability of a researcher to obtain
and use the data and analysis code from the author of published
scientific findings to reproduce those findings independently is an
essential standard by which the scientific community can judge the
integrity of the scientific record.</strong></p>
<p>Despite the vaccine-autism link having been disproven, Wakefield’s
research continues to be cited in anti-vaccine campaigns that spread
misinformation and disinformation about so-called dangers of vaccines.
Such campaigns have contributed to vaccine hesitancy, which has hindered
public health initiatives to reduce the spread of pandemic diseases
through vaccination. This example underscores the importance of recent
calls for reproducibility as a precondition of scientific publication.
<br><br><strong>Suggested readings:</strong><br>
Motta, M., &amp; Stecula, D. (2021). Quantifying the effect of Wakefield
et al. (1998) on skepticism about MMR vaccine safety in the U.S.
<em>PLOS ONE, 16</em>(8), e0256395. <a href="https://doi.org/10.1371/journal.pone.025639" class="external-link">https://doi.org/10.1371/journal.pone.0256395</a></p>
<p>Ullah, I., Khan, K. S., Tahir, M. J., Ahmed, A., &amp; Harapan, H.
(2021). Myths and conspiracy theories on vaccines and COVID-19:
Potential effect on global vaccine refusals. <em>Vacunas, 22</em>(2),
93–97. <a href="https://doi.org/10.1016/j.vacun.2021.01.001" class="external-link uri">https://doi.org/10.1016/j.vacun.2021.01.001</a></p>
</section><section><h2 class="section-heading" id="the-reproducibility-mandate">The Reproducibility Mandate<a class="anchor" aria-label="anchor" href="#the-reproducibility-mandate"></a>
</h2>
<hr class="half-width">
<p>There are plenty of high-profile examples of research that were found
to be irreproducible, prompting the question of whether or not science
is experiencing a “reproducibility crisis.” Recent studies to determine
the extent to which published research is or is not reproducible have
been concerning to many given that investigators were unable to
successfully reproduce findings from a significant portion of the
published research examined by investigators.</p>
<p>In a <a href="https://doi.org/10.1038/533452a" class="external-link">2016 survey conducted
by the journal <em>Nature</em></a>, researchers were asked if there was
a reproducibility crisis in science. Of the 1,576 who responded to the
survey, 90% agreed that there was at least a slight crisis. 70% conceded
that they were unable to reproduce a study conducted by another
scientist, with over half admitting to being unable to reproduce a study
that they, themselves, conducted!</p>
<p><strong>The reasons for irreproducibility are plenty, but the issues
that appear quite often are the result of scientific practices that
overlook the data management and curation activities</strong>: missteps
in analysis workflows due to gaps in documentation, ethical and/or legal
violations of non-anonymized human subjects data or redistribution of
restricted proprietary data, code execution failures because of
computing environment mismatches, inconsistent analysis outputs from use
of the wrong data file versions, and lack of access of data and code to
reproduce results. Despite these seemingly minor issues, the
repercussions can be serious.</p>
<div id="spotlight-yet-another-term-preproducibility" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-yet-another-term-preproducibility" class="callout-inner">
<h3 class="callout-title">Spotlight: Yet Another Term: “Preproducibility”</h3>
<div class="callout-content">
<p>In a 2018 article published in the journal <em>Nature</em>, Phillip
Stark offered another term for consideration for use in discussions of
reproducibility: <strong>preproducibility</strong>. He wrote, “An
experiment or analysis is preproducible if it has been described in
adequate detail for others to undertake it. Preproducibility is a
prerequisite for reproducibility…” (p. 613). The question of
reproducibility, according to Stark, is irrelevant if documentation of
research methods and protocols are insufficient or unavailable. Stark
declared his unequivocal stance on the subject, declining to review any
manuscript that is not preproducible.</p>
<p>Read the article that introduces the concept of preproducibility
here:<br><strong>Stark, P. B. (2018). Before reproducibility must come
preproducibility. <em>Nature, 557</em>(7707), 613–613. <a href="https://doi.org/10.1038/d41586-018-05256-0" class="external-link uri">https://doi.org/10.1038/d41586-018-05256-0</a></strong></p>
</div>
</div>
</div>
<p>Perhaps with the so-called reproducibility crisis in mind, various
research stakeholders have taken steps to promote and protect the
integrity of scientific research by issuing policies and/or guidelines
that require researchers to perform data management tasks to ensure data
are accessible and usable. This includes the funding agencies that
support research initiatives, scholarly journals that publish the
scientific record, and academic societies that establish standards for
research conduct.</p>
<div class="section level3">
<h3 id="funding-agencies">Funding Agencies<a class="anchor" aria-label="anchor" href="#funding-agencies"></a>
</h3>
<p>Funding agencies make billions of dollars in scientific investments
by sponsoring grant programs that provide support to research projects.
To maximize their investments, many funders have issued policies that
require grant awardees to make the materials produced in the course of
funded research activities publicly available. By doing so, the
scientific community can reuse datasets, analysis code, and other
research artifacts to extend and verify results. Below are examples of
funding agencies that have data policies in place.</p>
<table class="table">
<colgroup>
<col width="3%">
<col width="96%">
</colgroup>
<thead><tr class="header">
<th>Funding Agency</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://sloan.org/storage/app/media/files/application_documents/Sloan-Grant-Proposal-Guidelines-Research-Projects.pdf" class="external-link">Alfred
P. Sloan Foundation</a></td>
<td>“Scientific progress depends on the sharing of information, on the
replication of findings, and on the ability of every individual to stand
of the shoulders of her predecessors…potential grantees are asked to
attend to the outputs their research will create and how those outputs
can best be put in service to the larger scientific community.”</td>
</tr>
<tr class="even">
<td><a href="https://www.imls.gov/sites/default/files/digitalproduct.pdf" class="external-link">Institute
for Museum and Library Services (IMLS)</a></td>
<td>“The digital products you create with IMLS funding require effective
stewardship to protect and enhance their value, and they should be
freely and readily available for use and reuse by libraries, archives,
museums, and the public.”</td>
</tr>
<tr class="odd">
<td><a href="https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/data-rights-related-issues" class="external-link">NASA</a></td>
<td>“…scientific data product algorithms and data products or services
produced through the [Earth Science] program shall be made available to
the user community on a nondiscriminatory basis, without restriction,
and at no more than the marginal cost of fulfilling user requests.”</td>
</tr>
<tr class="even">
<td><a href="https://sharing.nih.gov/data-management-and-sharing-policy/about-the-data-management-sharing-policies" class="external-link">National
Institutes of Health (NIH)</a></td>
<td>“The National Institutes of Health (NIH) Policy for Data Management
and Sharing (herein referred to as the DMS Policy) reinforces NIH’s
longstanding commitment to making the results and outputs of NIH-funded
research available to the public through effective and efficient data
management and data sharing practices.”</td>
</tr>
<tr class="odd">
<td><a href="https://www.nsf.gov/bfa/dias/policy/dmp.jsp" class="external-link">National
Science Foundation (NSF)</a></td>
<td>“Investigators are expected to share with other researchers, at no
more than incremental cost and within a reasonable time, the primary
data, samples, physical collections and other supporting materials
created or gathered in the course of work under NSF grants. Grantees are
expected to encourage and facilitate such sharing.”</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="scholarly-journal-publishers">Scholarly Journal Publishers<a class="anchor" aria-label="anchor" href="#scholarly-journal-publishers"></a>
</h3>
<p>As stewards of the scientific record, journals bear responsibility
for ensuring that the articles they publish contain verifiable claims.
Moreover, journals recognize the role they play in traditional tenure
and promotion structures, which appraise scientific productivity based
on publication of articles in scholarly journals. Thus, journal policies
that make publication contingent on data sharing and verification are
considered a promising tool for advancing research reproducibility.
Below are examples of journals with rigorous data policies.</p>
<table class="table">
<colgroup>
<col width="3%">
<col width="96%">
</colgroup>
<thead><tr class="header">
<th>Scholarly Journal</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://ajps.org/ajps-verification-policy/" class="external-link">American
Journal of Political Science (AJPS)</a></td>
<td>“The corresponding author of a manuscript that is accepted for
publication in the <em>American Journal of Political Science</em> must
provide materials that are sufficient to enable interested researchers
to verify all of the analytic results that are reported in the text and
supporting materials…When the draft of the manuscript is submitted, the
materials will be verified to confirm that they do, in fact, reproduce
the analytic results reported in the article.”</td>
</tr>
<tr class="even">
<td><a href="https://www.aeaweb.org/journals/data/data-code-policy" class="external-link">American
Economic Association (AEA) Journals</a></td>
<td>“Authors of accepted papers that contain empirical work,
simulations, or experimental work must provide, prior to acceptance,
information about the data, programs, and other details of the
computations sufficient to permit replication, as well as information
about access to data and programs…The AEA Data Editor will assess
compliance with this policy, and will verify the accuracy of the
information prior to acceptance by the Editor.”</td>
</tr>
<tr class="odd">
<td><a href="https://reviewer.elifesciences.org/author-guide/journal-policies" class="external-link">eLife</a></td>
<td>“Regardless of whether authors use original data or are reusing data
available from public repositories, they must provide program code,
scripts for statistical packages, and other documentation sufficient to
allow an informed researcher to precisely reproduce all published
results.”</td>
</tr>
<tr class="even">
<td><a href="https://ps.psychopen.eu/index.php/ps/open-science" class="external-link">Personality
Science</a></td>
<td>“<em>Personality Science</em> (PS) takes good, transparent,
reproducible, and open science very seriously. This means that all
published papers will have underwent screening regarding to what extent
they have fulfilled Transparency and Openness Promotion (TOP)
Guidelines.”</td>
</tr>
<tr class="odd">
<td><a href="https://www.science.org/content/page/science-journals-editorial-policies#research-standards" class="external-link">Science</a></td>
<td>“All data used in the analysis must be available to any researcher
for purposes of reproducing or extending the analysis…In general, all
computer code central to the findings being reported should be available
to readers to ensure reproducibility…Materials/samples used in the
analysis must be made available to any researcher for purposes of
directly replicating the procedure.”</td>
</tr>
</tbody>
</table>
</div>
<div class="section level3">
<h3 id="academic-societies">Academic Societies<a class="anchor" aria-label="anchor" href="#academic-societies"></a>
</h3>
<p>Academic societies have taken up the issue of reproducibility in
updated professional codes of conduct or ethics. Citing responsibility
to advance research in their discipline, these formal documents obligate
researchers to enable others to evaluate their knowledge claims through
transparent research practices and public access to data and materials
underlying those knowledge claims. Below is a list of academic societies
that have issued policies or statements promoting data access and
research transparency.</p>
<table class="table">
<colgroup>
<col width="3%">
<col width="96%">
</colgroup>
<thead><tr class="header">
<th>Academic Society</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://www.agu.org/-/media/Files/AGU-Data-Position-Statement-Final-2015.pdf" class="external-link">American
Geophysical Union (AGU)</a></td>
<td>“The cost of collecting, processing, validating, documenting, and
submitting data to a repository should be an integral part of research
and operational programs. The AGU scientific community should recognize
the professional value of such activities.”</td>
</tr>
<tr class="even">
<td><a href="https://www.apa.org/ethics/code" class="external-link">American Psychological
Association (APA)</a></td>
<td>“After research results are published, psychologists do not withhold
the data on which their conclusions are based from other competent
professionals who seek to verify the substantive claims through
reanalysis and who intend to use such data only for that purpose,
provided that the confidentiality of the participants can be protected
and unless legal rights concerning proprietary data preclude their
release.”</td>
</tr>
<tr class="odd">
<td><a href="https://www.acm.org/code-of-ethics" class="external-link">American Sociological
Association (ASA)</a></td>
<td>“Consistent with the spirit of full disclosure of methods and
analyses, once findings are publicly disseminated, sociologists permit
their open assessment and verification by other responsible researchers,
with appropriate safeguards to protect the confidentiality of research
participants…As a regular practice, sociologists share data and
pertinent documentation as an integral part of a research plan.”</td>
</tr>
</tbody>
</table>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>The term ‘reproducibility’ has been used in different ways in
different disciplinary contexts.</li>
<li>Computational reproducibility, which is the focus of this and
follow-up lessons, refers to the duplication of reported findings by
re-executing the analysis with the data and code used by the original
author to generate their findings.</li>
<li>Scientific reproducibility is not a novel concept, but one that has
been reiterated by prominent scholars throughout history as a
cornerstone of scientific practice.</li>
<li>Failed attempts to reproduce published scientific research are
considered by some to be reflective of an ongoing crisis in scientific
integrity.</li>
<li>Stakeholders have taken note of the importance of reproducibility
and thus have issued policies requiring researchers to share their
research artifacts with the scientific community.</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-02-standards"><p>Content from <a href="02-standards.html">Reproducibility Standards</a></p>
<hr>
<p>Last updated on 2024-11-25 |

        <a href="https://github.com/ucla-data-science-center/lc-cure-01/edit/main/episodes/02-standards.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 45 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What makes research reproducible?</li>
<li>How does a research compendium support reproducibility?</li>
<li>What are common obstacles to making research reproducible?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain the requirements for meeting the reproducibility
standard.</li>
<li>Describe the contents of a research compendium.</li>
<li>Recall some of the challenges that can hinder efforts to make
research reproducible.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>In the previous episode, we established our working definition of
“reproducibility” as being able to reproduce published results given the
same data, code, and other research artifacts originally used to execute
the computational workflow. In this episode, we go beyond the definition
of reproducibility to explore the standards by which we
<em>evaluate</em> reproducibility.</p>
<section><h2 class="section-heading" id="reproducibility-standards">Reproducibility Standards<a class="anchor" aria-label="anchor" href="#reproducibility-standards"></a>
</h2>
<hr class="half-width">
<p>In 1995, the Harvard University professor of political science, Gary
King, proposed a new “replication standard,” which he considered
imperative to the discipline’s ability to understand, verify, and expand
its scholarship (here, King uses the term “replication,” but his use is
analogous to our working definition of reproducibility):</p>
<blockquote>
<p>The <em>replication standard</em> holds that sufficient information
exists with which to understand, evaluate, and build upon a prior work
if a third party could replicate the results without any additional
information from the author (<a href="https://doi.org/10.2307/420301" class="external-link">p. 444</a>).</p>
</blockquote>
<p>Accessibility and usability of the materials necessary to reproduce
reported research results without having to resort to intervention from
the original author is central to King’s standard, and, as you will
learn, are at the core of curation for reproducibility practices.
<br><br><strong>Read King’s seminal article here:</strong><br>
King, G. (1995). Replication, replication. <em>PS: Political Science
&amp; Politics, 28</em>(3), 444–452. <a href="https://doi.org/10.2307/420301" class="external-link uri">https://doi.org/10.2307/420301</a></p>
<div id="spotlight-fair-principles" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-fair-principles" class="callout-inner">
<h3 class="callout-title">Spotlight: FAIR Principles</h3>
<div class="callout-content">
<p>Mention of the FAIR Principles has become ubiquitous in discussions
of research data management. Some funding agencies have even cited the
FAIR Principles in documents that provide guidance for their data
management policies. FAIR, which stands for findable, accessible,
interoperable, and reusable, was developed as a set of guiding
principles that help to sustain and enhance the value of data for
scientific discovery, knowledge-making, and innovation, while avoiding
the technical obstacles that can challenge those goals.</p>
<p>In the summary of the FAIR Principles below, pay particular attention
to the Reusable principle, which when thinking about reproduction as one
type of reuse, aligns directly with some of the standards we uphold when
curating for reproducibility.</p>
<p><strong>Findable</strong><br><em>The data are uniquely identified and described using
machine-readable metadata to enable both systems and humans to find the
data in a searchable system.</em><br><strong>F1.</strong> (Meta)data are assigned a globally unique and
persistent identifier<br><strong>F2.</strong> Data are described with rich metadata (defined by
R1 below)<br><strong>F3.</strong> Metadata clearly and explicitly include the
identifier of the data they describe<br><strong>F4.</strong> (Meta)data are registered or indexed in a
searchable resource</p>
<p><strong>Accessible</strong><br><em>Access to the data (or descriptive metadata should the data no
longer be available or require certain procedures for access) using the
unique identifier assigned to the data is possible without the need for
specialized tools or services.</em><br><strong>A1.</strong> (Meta)data are retrievable by their identifier
using a standardized communications protocol<br><strong>A1.1</strong> The protocol is open, free, and universally
implementable<br><strong>A1.2</strong> The protocol allows for an authentication and
authorisation procedure, where necessary<br><strong>A2.</strong> Metadata are accessible, even when the data are no
longer available</p>
<p><strong>Interoperable</strong><br><em>The data and descriptive metadata are standardized to enable
exchange and interpretation of data by different people and
systems.</em><br><strong>I1.</strong> (Meta)data use a formal, accessible, shared, and
broadly applicable language for knowledge representation.<br><strong>I2.</strong> (Meta)data use vocabularies that follow FAIR
principles<br><strong>I3.</strong> (Meta)data include qualified references to other
(meta)data</p>
<p><strong>Reusable</strong><br><em>The data are presented with enough detail that it is clear to
designated users the origins of the data, how to interpret and use the
data appropriately, and by whom and for what purposes the data may be
used.</em><br><strong>R1.</strong> (Meta)data are richly described with a plurality of
accurate and relevant attributes<br><strong>R1.1.</strong> (Meta)data are released with a clear and
accessible data usage license<br><strong>R1.2.</strong> (Meta)data are associated with detailed
provenance<br><strong>R1.3.</strong> (Meta)data meet domain-relevant community
standards</p>
<p><strong>Read more about the FAIR Principles here:</strong><br>
GO FAIR. (n.d.). <em>FAIR Principles</em>. GO FAIR. Retrieved May 6,
2022, from <a href="https://www.go-fair.org/fair-principles/" class="external-link uri">https://www.go-fair.org/fair-principles/</a></p>
<p>Wilkinson, M. D., Dumontier, M., Aalbersberg, Ij. J., Appleton, G.,
Axton, M., Baak, A., Blomberg, N., Boiten, J.-W., da Silva Santos, L.
B., Bourne, P. E., Bouwman, J., Brookes, A. J., Clark, T., Crosas, M.,
Dillo, I., Dumon, O., Edmunds, S., Evelo, C. T., Finkers, R., … Mons, B.
(2016). The FAIR Guiding Principles for scientific data management and
stewardship. <em>Scientific Data, 3</em>, 160018. <a href="https://doi.org/10.1038/sdata.2016.18" class="external-link uri">https://doi.org/10.1038/sdata.2016.18</a></p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="the-research-compendium">The Research Compendium<a class="anchor" aria-label="anchor" href="#the-research-compendium"></a>
</h2>
<hr class="half-width">
<p>The first known mention of the research compendium concept appeared
in an article by <a href="https://doi.org/10.1198/106186007X178663" class="external-link">Gentleman and Lang
(2007)</a>, who proposed the compendium as a “new mechanism that
combines text, data, and auxiliary software into a distributable and
executable unit” (p. 2). Noting the challenges of assembling the
artifacts needed to re-run a statistical analysis to computationally
reproduce published research results, Gentleman and Lang argued the need
to capture the steps of the analytical workflow in a compendium
containing one or more “dynamic documents” that encapsulate the
description of the analysis (manuscript and documentation), the analysis
inputs (data and code), and the computing environment (computational and
analytical software).</p>
<p>To put it more simply for this Curating for Reproducibility
curriculum, <strong>we refer to the research compendium as the
collection of the research artifacts necessary to independently
understand and repeat the entirety of the analysis workflow from data
processing and transformation to producing results</strong>.</p>
<figure><img src="../fig/02-compendium.png" title="Research compendium components" alt="Research compendium components" class="figure mx-auto d-block"></figure><p>At a minimum, the research compendium, which may also referred to as
reproducibility file bundle or reproducibility package as we define it,
contains the following research artifacts:</p>
<ul>
<li>
<strong>Data.</strong> Dataset files including the raw or original
data, and constructed data used in the analysis.</li>
<li>
<strong>Code.</strong> Processing scripts used to construct analysis
datasets from raw or original data, and analysis scripts that generate
reported results.</li>
<li>
<strong>Documentation.</strong> Information that provides sufficient
information to enable re-execution of the analysis workflow including
data dictionaries, data availability statements for restricted data,
code execution instructions, computing environment specifications, and
other necessary details to reproduce results.</li>
</ul>
<p>The research compendium supports the reproducibility standard by
providing access to all of the information, materials and tools
necessary to independently reproduce the associated results. Curating
for reproducibility applies curation actions to the research
compendium.</p>
<div id="spotlight-curating-for-reproducibility-beyond-data" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-curating-for-reproducibility-beyond-data" class="callout-inner">
<h3 class="callout-title">Spotlight: Curating for Reproducibility: Beyond Data</h3>
<div class="callout-content">
<p>Curating for reproducibility requires that we think beyond data as
the singular object of curation. When considered alone, data that are
free of errors, clean of personally identifiable information, and
well-documented may be deemed high-quality.</p>
<p>In the context of reproducibility, however, quality standards are
applied to the research compendium, of which data is but one component.
If an attempt to independently reproduce reported results using the
compendium fails, then a quality assessment of the data alone is
insufficient.</p>
<p><strong>Curating for reproducibility considers the research
compendium as the object of curation</strong>, with the goal of ensuring
that all of the artifacts contained within it capture all of the
materials, information, computational workflows, and technical
specifications needed to re-execute the analysis to reproduce published
results.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="independently-understandable-for-informed-reuse">Independently Understandable for Informed Reuse<a class="anchor" aria-label="anchor" href="#independently-understandable-for-informed-reuse"></a>
</h2>
<hr class="half-width">
<p>Curation in support of scientific research has centered commonly on
data curation to support long-term access to and reuse of high-quality
datasets. It is useful to think about quality not so much as attributes
of the data, but as a “set of measures that determine if data are
independently understandable for informed reuse” (<a href="https://doi.org/10.2218/ijdc.v9i1.317" class="external-link">Peer et al., 2014</a>).</p>
<div class="section level3">
<h3 id="independent-understandability">Independent Understandability<a class="anchor" aria-label="anchor" href="#independent-understandability"></a>
</h3>
<p><strong>Independent understandability</strong> is a key curation
concept that refers to the ability of anyone unrelated to the production
of the data to interpret and use the data without needing input from the
original producer(s). That means that in order to enable reuse, data
need to be processed, documented, shared, and preserved in a way that
ensures that they are “independently understandable to (and usable by)
the Designated Community” (a Designated Community is “an identified
group of potential Consumers who should be able to understand a
particular set of information….[it] is defined by the Archive and this
definition may change over time,” (<a href="https://public.ccsds.org/publications/archive/650x0m2.pdf" class="external-link">CCSDS,
2012</a>)).</p>
</div>
<div class="section level3">
<h3 id="reusability">Reusability<a class="anchor" aria-label="anchor" href="#reusability"></a>
</h3>
<p>The other important concept in data curation is
<strong>reusability</strong>. Motivations for reuse of extant data are
varied and include data verification, new analysis, re-analysis, or
meta-analysis. Reproduction of original analysis and results, which is
the focus of this curriculum, is a type of reuse that sets an even
higher bar for quality because it requires that code and detailed
documentation–not only the data–to be curated and packaged into a
research compendium to allow regeneration of corresponding published
results.</p>
<div id="spotlight-10-things-for-curating-reproducible-and-fair-research" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-10-things-for-curating-reproducible-and-fair-research" class="callout-inner">
<h3 class="callout-title">Spotlight: 10 Things for Curating Reproducible and FAIR Research</h3>
<div class="callout-content">
<p>The measures used to determine the quality of a research compendium
intended to be used to reproduce reported results are described in the
<strong><em>10 Things for Curating Reproducible and FAIR
Research</em></strong>. The 10 Things, summarized below are the output
of the Research Data Alliance’s CURE-FAIR Working Group. This
international community of information professionals, researchers,
funding agencies, publishers, and others interested in promoting
reproducibility practices worked together to identify and describe
specific requirements for making research reproducible. <img src="../fig/02-10things.png" title="10 Things for Curating Reproducible and FAIR Research" alt="10 Things for Curating Reproducible and FAIR Research" class="figure"> CURE-FAIR
Working Group. (2022). 10 things for curating reproducible and FAIR
research. Research Data Alliance. <a href="https://doi.org/10.15497/RDA00074" class="external-link uri">https://doi.org/10.15497/RDA00074</a></p>
</div>
</div>
</div>
<p>Knowing what makes research reproducible allows us to recognize
research that is not reproducible. The next challenge revisits the
scenarios from the <a href="01-overview.html">challenge in the previous
episode</a> to get us thinking about the causes of irreproducibility and
their potential consequences.</p>
<blockquote>
<h2 id="exercise-the-impact-of-irreproducibility">Exercise: The Impact
of Irreproducibility</h2>
<p>Consider again the three scenarios from the exercise in <a href="01-overview.html">Episode 1 (“Reproducibility, Reproducibility,
Reproducibility”)</a>. Provide responses to the following questions
about each scenario.</p>
<ul>
<li><strong>What were the causes of non-reproducibility?</strong></li>
</ul>
</blockquote>
<ul>
<li><strong>What may have been some consequences of the discovery that
the study was not reproducible?</strong></li>
<li><strong>What could the researchers of the original study have done
differently to avoid the issues that rendered the research
non-reproducible?</strong></li>
</ul>
<div id="challenge1" class="callout challenge">
<div class="callout-square">
<i class="callout-icon" data-feather="zap"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Challenge</h3>
<div class="callout-content">

</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" aria-labelledby="headingSolution1" data-bs-parent="#accordionSolution1">
<div class="accordion-body">
<p><strong>Scenario 1 (Slicing and Dicing Food Data):</strong> All of
the research outputs from the food lab have been called into question
since the discovery of various problems in studies published by the PI
of the lab. The nutrition policies and programs in which the PI played a
part may need to be reconsidered in light of accusations of scientific
misconduct. The PI’s published research findings were not reproducible
because of his failure to provide access to documentation and data that
could demonstrate the integrity of his analytic methods and results.
Without it, his scientific claims are indefensible. Beyond providing
access to documentation, data, and code, pre-registration could have
been an effective strategy for preventing the problems seen in the lab’s
work and making results analytically reproducible. Pre-registering a
study obliges researchers to declare their hypothesis, study design, and
analysis plan prior to the start of research activities. Deviations from
the plan require documented explanation, which creates greater research
transparency.</p>
<p><strong>Scenario 2 (Excel Fail):</strong> Governments who implemented
austerity measures based on the findings in the original article may not
have yielded the expected outcomes and instead exacerbated the economic
crisis in their respective countries as a result. The causes of
non-reproducibility were primarily due to errors made in the Excel
spreadsheet used by the original authors for calculations. Rather than
rely on a spreadsheet program, the authors could have used statistical
software designed for data analysis to write and execute code to
generate results. The code itself reveals the analytic steps taken to
arrive at the reported results, which would have enabled the authors
(and secondary users) to inspect and verify the validity of their
analytic workflow and outputs and promote computational
reproducibility.</p>
<p><strong>Scenario 3 (Power(less) Pose):</strong> The publication of
the power pose research and the scrutiny it was met with happened in the
midst of heated arguments among psychologists about the scientific rigor
of research produced in their disciplinary domain. Some scholars in the
field took umbrage against published studies that presented seemingly
unlikely findings and set to work to assess the validity of these
findings. What they discovered was widespread abuse of researchers’
degrees of freedom as a means of generating positive, and likely more
publishable, results. Because of the publicity it received, this
research became something of a poster child of the so-called
reproducibility crisis in psychology with its questionable methods and
perhaps overstated positive results. This cast widespread doubt not only
on those particular research findings, but also on those from the field
of psychology writ large. Clear and comprehensive documentation and
justification of research protocols used in the power pose experiments
would have helped bolster the empirical reproducibility of the research
claims.</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="challenges-to-reproducibility">Challenges to Reproducibility<a class="anchor" aria-label="anchor" href="#challenges-to-reproducibility"></a>
</h2>
<hr class="half-width">
<p>While much of the research community and its stakeholders have
reached the consensus that reproducibility is imperative to the
scientific enterprise, it must be acknowledged the challenges that come
with efforts to make research reproducible. Indeed for some situations,
reproducibility is much easier said than done.</p>
<div class="section level3">
<h3 id="human-subjects-research">Human Subjects Research<a class="anchor" aria-label="anchor" href="#human-subjects-research"></a>
</h3>
<p>Research that involves human subjects are bound by laws and
regulations that place restrictions on data sharing, disallowing their
dissemination for any purpose to include verifying the reproducibility
of the results of that research. To avoid sanctions from unknowingly
failing to protect the identities of study participants, researchers
will often decline to share the data or opt to destroy the data upon
completion of their analyses.</p>
</div>
<div class="section level3">
<h3 id="copyright-and-intellectual-property-rights">Copyright and Intellectual Property Rights<a class="anchor" aria-label="anchor" href="#copyright-and-intellectual-property-rights"></a>
</h3>
<p>As with human subjects research, investigations that use proprietary
data and/or code from commercial sources often cannot be redistributed
because they are subject to licensing restrictions or intellectual
property laws that disallow doing so. Since access to the data and
materials underlying research results is a basic requirement for
substantiating published claims, proprietary or otherwise restricted
data stand as an obstacle to research reproducibility.</p>
</div>
<div class="section level3">
<h3 id="technological-hurdles">Technological Hurdles<a class="anchor" aria-label="anchor" href="#technological-hurdles"></a>
</h3>
<p>Despite having the same data, code, and other research materials used
by the original investigator, attempts to computationally reproduce
scientific results can prove quite difficult when the technology needed
to re-execute the analysis presents challenges. Some analyses require
computing environments with exacting specifications that are difficult
to recreate. Others are resource-intensive, demanding the processing
power of high-performance computing that may not be readily
accessible.</p>
</div>
<div class="section level3">
<h3 id="time-and-effort">Time and Effort<a class="anchor" aria-label="anchor" href="#time-and-effort"></a>
</h3>
<p>Reproducible research requires the availability of code written using
literate programming and includes non-executable comments indicating the
function of code blocks; datasets accompanied by codebooks or data
dictionaries that define each variable and its categorical value codes;
readme files that describe the process for executing the analysis; and
any other materials necessary to independently execute the analysis to
generate outputs identical to the original results. Preparing these
materials may take a great deal of time and effort that researchers just
might not have, especially when there is little or no incentive to do
so.</p>
<p><br> Are these challenges insurmountable? Not always. In many cases,
strategies exist that account for these challenges to make it possible
to still uphold reproducibility standards. In <a href="">Lesson 4:
Compendium Packaging</a> and throughout the Curating for Reproducibility
Curriculum, you will learn about these strategies and how to apply them
to special cases involving technical complexities, sensitive human
subjects data, and restricted proprietary data.</p>
<div id="spotlight-a-reproducibility-dare" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-a-reproducibility-dare" class="callout-inner">
<h3 class="callout-title">Spotlight: A Reproducibility Dare</h3>
<div class="callout-content">
<p>When Nicolas Rougier and Konrad Hinsen, the originators of the <a href="https://rescience.github.io/ten-years/" class="external-link">Ten Years Reproducibility
Challenge</a>, issued an invitation to researchers to find the code they
used to generate results presented in any article published before 2010
and then use the unedited code to reproduce the results, they suspected
that few would succeed.</p>
<figure><img src="../fig/02-10years-challenge.png" title="Ten Years Reproducibility Challenge flyer" alt="Ten Years Reproducibility Challenge" class="figure mx-auto d-block"></figure><p>Ten years is considered an eternity when it comes to the longevity of
computations. Rapidly changing technologies render hardware and software
obsolete, and evolving computational approaches relegate once novel
programming languages to outdated status–in significantly less time than
ten years. The 35 entrants confirmed this to be the case, having to
resort to using hardware emulators or purchases from online vendors to
obtain old hardware, reaching into the depths of memory to revive fading
programming language fluency, and confronting poor coding practices that
have since been remedied by subsequent years of experience. The
difficulty of the challenge cannot be overstated.</p>
<p>One crucial takeaway that participants noted was the importance of
documentation to preserve the critical information about where to locate
the code, what hardware and software are needed to replicate the
original computing environment, and how to run the code to successfully
generate expected outputs. <strong>While constantly evolving technology
can make reproducibility elusive, comprehensive documentation is the key
to making it at all possible.</strong></p>
<p><strong>Learn more about the Ten Years Reproducibility Challenge
here:</strong><br> Perkel, J. M. (2020). Challenge to scientists: Does
your ten-year-old code still run? <em>Nature, 584</em>(7822), 656–658.
<a href="https://doi.org/10.1038/d41586-020-02462-7" class="external-link uri">https://doi.org/10.1038/d41586-020-02462-7</a></p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Reproducible research requires access to a “research compendium”
that contains all of the artifacts and documentation necessary to repeat
the steps of the analytical workflow to produce expected results.</li>
<li>Curating for reproducibility goes beyond curating data; it applies
curation actions to all of the research artifacts within the research
compendium to ensure it is independently understandable for informed
reuse.</li>
<li>Despite calls for reproducible research, challenges exist that can
make it difficult to achieve this standard.</li>
</ul>
</div>
</div>
</div>
</div>
</section></section><section id="aio-03-LIS"><p>Content from <a href="03-LIS.html">Scientific Reproducibility and the LIS Professional</a></p>
<hr>
<p>Last updated on 2024-11-25 |

        <a href="https://github.com/ucla-data-science-center/lc-cure-01/edit/main/episodes/03-LIS.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 45 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>What is the role of the LIS professional in supporting reproducible
research?</li>
<li>What are the requisite skills and knowledge for executing data
curation for reproducibility workflows?</li>
<li>In what ways can data curation for reproducibility activities be
incorporated into services?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain how curation for reproducibility differs from common models
of data curation that focuses on data as the object of curation.</li>
<li>Describe what it means to be a data savvy librarian.</li>
<li>Provide examples of curation for reproducibility service
implementation.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<p>There are several stakeholders that contribute to reproducibility.
The researcher who incorporates data management activities in their
research workflows, the funding agencies who mandate data sharing, the
repository that provides a platform for making the data available, and
the journal editor who checks that authors provide information on how to
access their data all play a role in promoting reproducibility. This
episode explores why library and information science (LIS) professionals
are also an important part of this endeavor and how they can support
it.</p>
<section><h2 class="section-heading" id="the-role-of-the-lis-professional">The Role of the LIS Professional<a class="anchor" aria-label="anchor" href="#the-role-of-the-lis-professional"></a>
</h2>
<hr class="half-width">
<p>Researchers are becoming more aware of services that support
reproducibility standards, many of which fall within the domain of the
LIS professional. They have made note that “libraries, with their
long-standing tradition of organization, documentation, and access, have
a role to play in supporting research transparency and preserving…‘the
research crown jewels’” (<a href="https://doi.org/10.2218/ijdc.v12i1.530" class="external-link">Lyon, Jeng, &amp; Mattern,
2017, p. 57</a>). Indeed, appraising the value of materials, arranging
and describing them, creating standardized metadata, assigning unique
identifiers, and other common library and archives tasks are applicable
to curation for reproducibility.</p>
<p>It is important to acknowledge that accepting this type of role
introduces additional demands on the LIS toolbox that calls for some
degree of subject knowledge and technical skills to engage in rigorous
data curation activities that support sustained access and use of
high-quality research materials to promote scientific reproducibility
i.e., data curation for reproducibility.</p>
<p>The <strong>“data savvy librarian”</strong> is one who can execute
data curation for reproducibility workflows because they:</p>
<ul>
<li>can apply the fundamentals of digital preservation to ensure that
research materials are discoverable, accessible, understandable, and
reusable into the future;</li>
<li>have the technical skills to assess the quality of research
materials produced by computational research projects; and</li>
<li>are familiar with the research lifecycle including the methods,
workflows, and tools of the disciplinary domain in which data are
collected or generated, transformed, and analyzed.</li>
</ul>
<p>The diagram below illustrates how various areas of skills and
knowledge apply to primary components of data curation for
reproducibility as they are defined by the Data Quality Review Framework
(see Lesson 2 for an in depth discussion of each component of the Data
Quality Review Framework).</p>
<figure><img src="../fig/03-cure-skills.png" title="Diagram of skills and knowledge necessary to perform data curation for reproducibility tasks" alt="Diagram of skills and knowledge necessary to perform data curation for reproducibility tasks" class="figure mx-auto d-block"></figure><p><strong>(Optional) Read more about the data savvy librarian and their
role in supporting scientific reproducibility:</strong><br>
Barbaro, A. (2016). On the importance of being a data-savvy librarian.
<em>JEAHIL, 12</em>(1), 25–27. <a href="https://ojs.eahil.eu/ojs/index.php/JEAHIL/article/view/100/104" class="external-link">http://ojs.eahil.eu/ojs/index.php/JEAHIL/article/view/100/104</a></p>
<p>Burton, M., Lyon, L., Erdmann, C., &amp; Tijerina, B. (2018).
Shifting to data savvy: The future of data science in libraries.
University of Pittsburgh. <a href="https://d-scholarship.pitt.edu/id/eprint/33891" class="external-link">http://d-scholarship.pitt.edu/id/eprint/33891</a></p>
<p>Kouper, I., Fear, K., Ishida, M., Kollen, C., &amp; Williams, S. C.
(2017). Research data services maturity in academic libraries. In
<em>Curating research data: Practical strategies for your digital
repository</em> (pp. 153–170). Association of College and Research
Libraries. <a href="https://doi.org/10.14288/1.0343479" class="external-link">https://doi.library.ubc.ca/10.14288/1.0343479</a></p>
<p>Lyon, L., Jeng, W., &amp; Mattern, E. (2017). Research transparency:
A preliminary study of disciplinary conceptualisation, drivers, tools
and support services. <em>International Journal of Digital Curation,
12</em>(1), 46. <a href="https://doi.org/10.2218/ijdc.v12i1.530" class="external-link uri">https://doi.org/10.2218/ijdc.v12i1.530</a></p>
<p>Pryor, G., &amp; Donnelly, M. (2009). Skilling up to do data: Whose
role, whose responsibility, whose career? <em>International Journal of
Digital Curation, 4</em>(2), 158–170. <a href="https://doi.org/10.2218/ijdc.v4i2.105" class="external-link uri">https://doi.org/10.2218/ijdc.v4i2.105</a></p>
<p>Sawchuk, S. L., &amp; Khair, S. (2021). Computational
reproducibility: A practical framework for data curators. <em>Journal of
eScience Librarianship, 10</em>(3), 1206. <a href="https://doi.org/10.7191/jeslib.2021.1206" class="external-link uri">https://doi.org/10.7191/jeslib.2021.1206</a></p>
<div id="spotlight-cure-career-pathways" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="spotlight-cure-career-pathways" class="callout-inner">
<h3 class="callout-title">Spotlight: CuRe Career Pathways</h3>
<div class="callout-content">
<p>Curating for reproducibility requires a set of skills that are rare
for any one person to have. <a href="https://doi.org/10.2218/ijdc.v4i2.105" class="external-link">Pryor and Donnelly
(2009)</a> remarked that careers in data curation are often “accidental”
in the absence of established career pathways. Indeed, how people obtain
the skills to fill professional roles that include data curation for
reproducibility responsibilities can be very different. Consider these
examples:</p>
<ul>
<li><p>A researcher with years of experience engaged in computational
science comes to understand the importance of data curation after
responding to data sharing demands from journals, funding agencies, and
other researchers. Experiencing the benefits of managing their data to
enable sharing, the researcher makes a career transition to become a
data manager for a research lab. In this new role, they use their domain
expertise to communicate effectively with researchers as they
incorporate data curation activities into the lab’s research
workflows.</p></li>
<li><p>A librarian, who took several graduate-level courses in digital
archives and records management, works for an academic library that is
expanding its research support services. Because of the relevance of
knowledge gained from their graduate studies, the librarian is assigned
a data curation role that includes performing quality review and
ingesting research data into the institutional repository. After
engaging with researchers to understand their data needs and taking
classes in statistical software, the librarian is able to include
curation for reproducibility activities into the repository ingest
workflow.</p></li>
<li><p>Among the editorial staff of some scholarly journals is a data
editor, who is responsible for enforcing the journal’s strict data
policies that require authors to submit their research compendium for
review prior to article publication. The data editor uses their domain
expertise, computational skills, and understanding of data quality
standards to evaluate results for reproducibility. In this role, the
data editor also provides guidance to authors to encourage them to
curate their research compendium before submitting them for
review.</p></li>
</ul>
<p>Regardless of the path that leads one to a career that involves
curation for reproducibility, they are part of an emerging workforce
equipped with a unique set of skills that are highly sought by
scientific stakeholders that demand reproducible research.</p>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="reproducibility-services">Reproducibility Services<a class="anchor" aria-label="anchor" href="#reproducibility-services"></a>
</h2>
<hr class="half-width">
<p>Founded in 1947, the Roper Center is considered to be one of the
earliest examples of an institution formalizing specific activities
around data preservation and dissemination. Despite growth in the number
of organizations dedicated to providing long-term access to research
data assets, data curation is still early in its maturity as an
established discipline. Data curation for reproducibility is an even
more undeveloped area, with few individuals and groups actively engaged
in the practice.</p>
<p>Those that have implemented data curation for reproducibility
services can serve as models for academic libraries, data repositories,
research institutions, and other groups planning to expand their
services to support reproducibility. The three institutions highlighted
below offer examples of how data curation for reproducibility services
can be delivered.</p>
<div class="section level3">
<h3 id="institution-for-social-and-political-studies-yale-university">Institution for Social and Political Studies, Yale University<a class="anchor" aria-label="anchor" href="#institution-for-social-and-political-studies-yale-university"></a>
</h3>
<p>The Institution for Social and Policy Studies (ISPS) was established
in 1968 by the Yale Corporation as an interdisciplinary center at the
university to facilitate research in the social sciences and public
policy arenas. ISPS is an independent academic unit within the
university, including affiliates from across the social sciences. ISPS
hosts its own digital repository meant to capture and preserve the
intellectual output of and the research produced by scholars affiliated
with ISPS, and strives to serve as a model for sharing and preserving
research data by implementing the ideals of scientific reproducibility
and transparency.</p>
<p>Datasets housed in the ISPS Data Archive have undergone a rigorous
ingest process that combines data curation with data quality review to
ensure materials meet quality standards that support computational
reproducibility. The process is managed by the Yale Application for
Research Data workflow tool, which structures and tracks curation and
review activities to generate high quality data packages that are
repository-agnostic.</p>
</div>
<div class="section level3">
<h3 id="cornell-center-for-social-sciences-cornell-university">Cornell Center for Social Sciences, Cornell University<a class="anchor" aria-label="anchor" href="#cornell-center-for-social-sciences-cornell-university"></a>
</h3>
<p>The Cornell Center for Social Sciences founded in 1981, anticipates
and supports the evolving computational and data needs of Cornell social
scientists and economists throughout the entire research process and
data lifecycle. CCSS is home to one of the oldest university-based
social science data archives in the United States that contains an
extensive collection of public and restricted numeric data files in the
social sciences with particular emphasis on demography, economics and
labor, political and social behavior, family life, and health.</p>
<p>CCSS also offers a Data Curation and Reproduction of Results Service,
R-squared or R2, where researchers with papers ready to submit for
publication can send their data and code to CCSS prior to submission for
appraisal, curation and replication. This is to ensure that published
results are replicable; and that data and codes are well documented,
reusable, packaged, and preserved in a trustworthy data repository for
access by current and future generations of researchers.</p>
</div>
<div class="section level3">
<h3 id="odum-institute-for-research-in-social-science-university-of-north-carolina-at-chapel-hill">Odum Institute for Research in Social Science, University of North
Carolina at Chapel Hill<a class="anchor" aria-label="anchor" href="#odum-institute-for-research-in-social-science-university-of-north-carolina-at-chapel-hill"></a>
</h3>
<p>The Odum Institute for Research in Social Science at the University
of North Carolina at Chapel Hill provides education and support for
research planning, implementation, and dissemination. The Odum Institute
hosts the UNC Dataverse, which provides open access to curated
collections of social science research datasets, while also serving as a
repository platform for researchers to preserve, share, and publish
their data.</p>
<p>While the Odum Institute Data Archive has been curating research data
to support discovery, access, and reuse since 1969, the archive has
recently expanded its service model to include comprehensive data
quality review. The Odum Institute provides this data review service to
journals that wish to add a verification component to their data sharing
policies. The Odum Institute model of curating data for reproducibility
as a cost-based service to journals exemplifies a convergence of
stakeholders around the principles of research transparency and
reproducibility.</p>
<div id="discussion-cure-implementers" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="discussion-cure-implementers" class="callout-inner">
<h3 class="callout-title">Discussion: CuRe Implementers</h3>
<div class="callout-content">
<p>Visit the website for an organization below or any other organization
that has implemented data curation for reproducibility services and/or
workflows. Based on the information presented on the website, discuss
the following:</p>
<ul>
<li><strong>Overall mission of the organization</strong></li>
<li><strong>How the service supports that mission</strong></li>
<li><strong>Who within the organization provides the
service</strong></li>
<li><strong>The audience to whom the service is targeted</strong></li>
</ul>
<p><strong>American Economics Association, Data Editor</strong><br><a href="https://aeadataeditor.github.io/aea-de-guidance/" class="external-link uri">https://aeadataeditor.github.io/aea-de-guidance/</a></p>
<p><strong>Certification Agency for Scientific Code and Data
(cascad)</strong><br><a href="https://www.cascad.tech/" class="external-link uri">https://www.cascad.tech/</a></p>
<p><strong>CODECHECK, University of Twente</strong><br><a href="https://www.itc.nl/research/open-science/codecheck/" class="external-link uri">https://www.itc.nl/research/open-science/codecheck/</a></p>
<p><strong>Cornell Center for Social Sciences: Results Reproduction
(R-squared) Service</strong><br><a href="https://socialsciences.cornell.edu/research-support/R-squared" class="external-link uri">https://socialsciences.cornell.edu/research-support/R-squared</a></p>
<p><strong>Institution for Social and Policy Studies, Yale
University</strong><br><a href="https://isps.yale.edu/research/data" class="external-link uri">https://isps.yale.edu/research/data</a></p>
<p><strong>Validation by The Science Exchange</strong><br><a href="https://validation.scienceexchange.com/#/home" class="external-link">http://validation.scienceexchange.com/#/home</a></p>
<p><strong>Smathers Libraries, University of Florida</strong><br><a href="https://arcs.uflib.ufl.edu/services/reproducibility/" class="external-link uri">https://arcs.uflib.ufl.edu/services/reproducibility/</a></p>
</div>
</div>
</div>
<p>Talking to others about the importance and benefits of curating for
reproducibility can be intimidating when put on the spot, especially
when asked to speak about it for the first time. Taking time to think
through what services look like or could look like at your institution
can go a long way to articulating your ideas effectively. Be strategic,
have fun, and get others as excited with your understanding and
commitment to reproducibility by meeting others where they are.</p>
<div id="exercise-elevator-pitch" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="exercise-elevator-pitch" class="callout-inner">
<h3 class="callout-title">Exercise: Elevator Pitch</h3>
<div class="callout-content">
<p>Using what you have learned about reproducibility, its importance and
how the LIS profession is well situated to support researchers with
curating for reproducibility, spend 5-10 minutes drafting an elevator
pitch about piloting a service to your colleague, supervisor, or dean.
What will you want to get across with just a few minutes of their time?
As you draft your pitch consider the following:</p>
<ul>
<li><strong>How does the service fit into your organization’s strategic
plan?</strong></li>
<li><strong>What value will it bring to the organization?</strong></li>
<li><strong>Who are the stakeholders providing the service as well as
receiving the service?</strong></li>
</ul>
<p>After you draft your pitch, practice saying it out loud a few times.
The next time you have a chance to advocate for reproducibility, you
will be ready!</p>
</div>
</div>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Data savvy librarians and other information professionals play an
important role in supporting and promoting scientific
reproducibility.</li>
<li>While LIS professionals already engage in many practices that
support reproducibility, they may need to skill up to perform some
critical curation for reproducibility tasks.</li>
<li>There are various models of data curation for implementation
services. It is important to think about what a service might look like
at your organization so that you can articulate your ideas effectively
when given the opportunity.</li>
</ul>
</div>
</div>
</div>
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/ucla-data-science-center/lc-cure-01/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/ucla-data-science-center/lc-cure-01/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/ucla-data-science-center/lc-cure-01/" class="external-link">Source</a></p>
				<p><a href="https://github.com/ucla-data-science-center/lc-cure-01/blob/main/CITATION" class="external-link">Cite</a> | <a href="mailto:team@carpentries.org">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.11" class="external-link">sandpaper (0.16.11)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.9" class="external-link">pegboard (0.7.9)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.5" class="external-link">varnish (1.0.5)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://curating4reproducibility.github.io/lc-cure-01-transition/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "software, data, lesson, The Carpentries",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://curating4reproducibility.github.io/lc-cure-01-transition/instructor/aio.html",
  "identifier": "https://curating4reproducibility.github.io/lc-cure-01-transition/instructor/aio.html",
  "dateCreated": "2022-06-20",
  "dateModified": "2025-03-14",
  "datePublished": "2025-03-14"
}

  </script><script>
		feather.replace();
	</script><!-- Matomo --><script>
          var _paq = window._paq = window._paq || [];
          /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
          _paq.push(["setDocumentTitle", document.domain + "/" + document.title]);
          _paq.push(["setDomains", ["*.lessons.carpentries.org","*.datacarpentry.github.io","*.datacarpentry.org","*.librarycarpentry.github.io","*.librarycarpentry.org","*.swcarpentry.github.io", "*.carpentries.github.io"]]);
          _paq.push(["setDoNotTrack", true]);
          _paq.push(["disableCookies"]);
          _paq.push(["trackPageView"]);
          _paq.push(["enableLinkTracking"]);
          (function() {
              var u="https://matomo.carpentries.org/";
              _paq.push(["setTrackerUrl", u+"matomo.php"]);
              _paq.push(["setSiteId", "1"]);
              var d=document, g=d.createElement("script"), s=d.getElementsByTagName("script")[0];
              g.async=true; g.src="https://matomo.carpentries.org/matomo.js"; s.parentNode.insertBefore(g,s);
          })();
        </script><!-- End Matomo Code -->
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

